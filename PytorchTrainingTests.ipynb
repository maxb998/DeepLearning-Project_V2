{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Notebook #\n",
    "This notebook will be used to test training of neural networks using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import non_max_iou_suppression, mean_average_precision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 2023\n",
    "#torch.manual_seed(2023)\n",
    "\n",
    "do_training = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "learning_rate = 2e-5\n",
    "batch_size = 10 # mini-batch is always 1 so this only affects vram usage\n",
    "weight_decay = 0\n",
    "epochs = 150\n",
    "num_workers = 3\n",
    "pin_memory = True\n",
    "\n",
    "# loss hyperparameter (they influence each other)\n",
    "lambda_coord = 5     # multiplier to the loss part relative to the coordinates of the bounding boxes(needs to be high probably)\n",
    "lambda_no_obj = 0.05  # multiplier to the loss part that regulates the fact that the network detects an objects where there isn't one\n",
    "# Also to note that there is also the class_loss component and obj_loss component(the opposite of the no_obj loss) which multiplier is 1 and they are still influenced by lambda_coord and lambda_no_obj\n",
    "\n",
    "# inference hyperparameters\n",
    "confidence_threshold = 0.6 # highly affects time to detect(and detection itself). If the influence of no_obj_loss is too weak than this must be set low, otherwise high\n",
    "iou_threshold = 0.7 # affects how many \"duplicate\" bounding boxes are merged during non-max suppression\n",
    "\n",
    "dataset_path = \"./data/RisikoDataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "Setup dataset class and dataset object. The output of the getitem function of the dataset is a tuple of two tensors, one for the image values (converted to float and scaled so that each pixel value range is [0,1)]) and one for the true label of the objects.\n",
    "This last one is a tensor of dimensions [num_of_objects, 5] and it is structured like this: [x_center, y_center, width, height, obj_class].\n",
    "\n",
    "Alternatively upon calling getitem, one might specify mode_plot=true which alters the first part of the output, returning the image in PIL format instead of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RisikoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir:str, mode:str, transform=None):\n",
    "        if mode != \"train\" and mode != \"val\" and mode != \"test\" and mode != \"real\":\n",
    "            raise Exception(\"Mode value of dataset not valid\")\n",
    "\n",
    "        self.imgs_dir = dataset_dir + \"/\" + mode + \"/images\"\n",
    "        self.annots_dir = dataset_dir + \"/\" + mode + \"/labels\"\n",
    "\n",
    "        self.annotations = sorted( filter( lambda x: os.path.isfile(os.path.join(self.annots_dir, x)), os.listdir(self.annots_dir) ) )\n",
    "        self.images = sorted( filter( lambda x: os.path.isfile(os.path.join(self.imgs_dir, x)), os.listdir(self.imgs_dir) ) )\n",
    "        self.transform = transform\n",
    "\n",
    "        offsets_1 = torch.stack([torch.arange(0, 1, 1/128, dtype=torch.float32).repeat(72), torch.arange(0, 1, 1/72, dtype=torch.float32).repeat(128, 1).t().flatten()]).t()\n",
    "        offsets_2 = torch.stack([torch.arange(1/128, 1.0001, 1/128, dtype=torch.float32).repeat(72), torch.arange(1/72, 1.0001, 1/72, dtype=torch.float32).repeat(128, 1).t().flatten()]).t()\n",
    "        self.grid_boxes = torch.cat([offsets_1, offsets_2], 1)\n",
    "\n",
    "        if len(self.annotations) != len(self.images):\n",
    "            raise Exception(\"Number of annotations is different from the number of images\")\n",
    "\n",
    "        for i in range(len(self.annotations)):\n",
    "            if os.path.splitext(os.path.basename(self.annotations[i]))[0] != os.path.splitext(os.path.basename(self.images[i]))[0]:\n",
    "                raise Exception(\"Mismatch between images and annotations at id \" + str(i) + \".   imgName = \" + os.path.splitext(os.path.basename(self.images[i]))[0] + \"   labelName = \" + os.path.splitext(os.path.basename(self.annotations[i]))[0])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx:int, mode_plot:bool=False) -> tuple[torch.Tensor, torch.tensor]:\n",
    "        annotations_file_data = np.genfromtxt(fname= self.annots_dir + \"/\" + self.annotations[idx], delimiter=' ', dtype=np.float32)\n",
    "        classes, bboxes = np.hsplit(annotations_file_data, np.array([1]))\n",
    "        #classes[:] = 0 #to check if there are issues with classification\n",
    "\n",
    "        basic_annotations = torch.cat([torch.from_numpy(bboxes), torch.from_numpy(classes)], 1)\n",
    "        \n",
    "        img = Image.open(self.imgs_dir + \"/\" + self.images[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform: img = self.transform(img)\n",
    "        \n",
    "        if mode_plot: return img, basic_annotations\n",
    "\n",
    "        pil_to_tensor = transforms.Compose([transforms.PILToTensor()])\n",
    "        img:torch.Tensor = pil_to_tensor(img)\n",
    "\n",
    "        # normalize image from 0 to 1\n",
    "        img = img.to(torch.float32) / 256\n",
    "\n",
    "        annotations = torch.ones([300,5], dtype=torch.float32) * -1\n",
    "        annotations[:basic_annotations.size()[0], ...] = basic_annotations\n",
    "\n",
    "        return img, annotations\n",
    "\n",
    "def shorten_annotations_tensor(annnotations:torch.Tensor) -> torch.Tensor:\n",
    "    return annnotations[annnotations != -1].reshape([-1,5])\n",
    "\n",
    "\n",
    "train_set = RisikoDataset(dataset_dir=dataset_path, mode=\"train\")\n",
    "val_set = RisikoDataset(dataset_dir=dataset_path, mode=\"val\")\n",
    "test_set = RisikoDataset(dataset_dir=dataset_path, mode=\"test\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False, num_workers=1, pin_memory=pin_memory, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1, pin_memory=pin_memory, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset check ###\n",
    "Print random image with bouding box to be sure that everything is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_on_image(dataset: RisikoDataset, index:int):\n",
    "    img, labels = dataset.__getitem__(index, mode_plot=True)\n",
    "\n",
    "    labels = shorten_annotations_tensor(labels)\n",
    "\n",
    "    bboxes = labels[..., 0:4]\n",
    "\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    bboxes = bboxes * torch.tensor([1280,720,1280,720])\n",
    "\n",
    "    for i in range(bboxes.shape[0]):\n",
    "        bbox = bboxes[i]\n",
    "\n",
    "        x0 = bbox[0] - bbox[2] / 2\n",
    "        x1 = bbox[0] + bbox[2] / 2\n",
    "        y0 = bbox[1] - bbox[3] / 2\n",
    "        y1 = bbox[1] + bbox[3] / 2\n",
    "\n",
    "        img_draw.rectangle([x0, y0, x1, y1], outline=\"red\")\n",
    "        \n",
    "    display(img)\n",
    "\n",
    "    img, labels = dataset.__getitem__(index)\n",
    "    print(img)\n",
    "    print(labels)\n",
    "\n",
    "\n",
    "draw_bboxes_on_image(train_set, random.randint(0, len(train_set)-1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##\n",
    "Definition of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 3)\n",
    "        self.conv3 = nn.Conv2d(128, 256, (3,5))\n",
    "        self.conv4 = nn.Conv2d(256, 512, (3,5))\n",
    "        self.conv5 = nn.Conv2d(512, 512, (3,5))\n",
    "        self.conv6 = nn.Conv2d(512, 256, 3)\n",
    "        self.conv7 = nn.Conv2d(256, 64, 3, padding=1)\n",
    "        self.finalConv = nn.Conv2d(64, 12+1+4, 3, padding=1) # 12 for classes, 1 for obj presence prob. and 4 for bbox\n",
    "        self.scale = torch.ones(17, dtype=torch.float32)\n",
    "        self.scale[13:15] = torch.tensor([1/128, 1/72])\n",
    "        self.scale = self.scale.expand(128*72,17).to(device)\n",
    "        self.center_offset = torch.zeros([128*72,17], dtype=torch.float32)\n",
    "        self.center_offset[..., 13:15] = torch.stack([torch.arange(0, 1, 1/128, dtype=torch.float32).repeat(72), torch.arange(0, 1, 1/72, dtype=torch.float32).repeat(128, 1).t().flatten()]).t()\n",
    "        self.center_offset = self.center_offset.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        x = F.leaky_relu(self.conv7(x))\n",
    "        x = F.sigmoid(self.finalConv(x))\n",
    "        x = x.flatten(start_dim=-2)\n",
    "        x = x.transpose(-2,-1)\n",
    "        x = x * self.scale + self.center_offset\n",
    "\n",
    "        return x\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 3)\n",
    "        self.conv3 = nn.Conv2d(128, 128, (3,5))\n",
    "        self.conv4 = nn.Conv2d(128, 256, (3,5))\n",
    "        self.conv5 = nn.Conv2d(256, 128, (3,5))\n",
    "        self.conv6 = nn.Conv2d(128, 64, 3)\n",
    "        #self.conv7 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.finalConv = nn.Conv2d(64, 12+1+4, 3, padding=1) # 12 for classes, 1 for obj presence prob. and 4 for bbox\n",
    "        self.scale = torch.ones(17, dtype=torch.float32)\n",
    "        self.scale[13:15] = torch.tensor([1/128, 1/72])\n",
    "        self.scale = self.scale.expand(128*72,17).to(device)\n",
    "        self.center_offset = torch.zeros([128*72,17], dtype=torch.float32)\n",
    "        self.center_offset[..., 13:15] = torch.stack([torch.arange(0, 1, 1/128, dtype=torch.float32).repeat(72), torch.arange(0, 1, 1/72, dtype=torch.float32).repeat(128, 1).t().flatten()]).t()\n",
    "        self.center_offset = self.center_offset.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = F.leaky_relu(self.conv6(x))\n",
    "        #x = F.leaky_relu(self.conv7(x))\n",
    "        x = F.sigmoid(self.finalConv(x))\n",
    "        x = x.flatten(start_dim=-2)\n",
    "        x = x.transpose(-2,-1)\n",
    "        x = x * self.scale + self.center_offset\n",
    "\n",
    "        return x\n",
    "\n",
    "net = SmallNet().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, labels = train_set.__getitem__(0)\n",
    "labels = shorten_annotations_tensor(labels)\n",
    "img = img.to(device)\n",
    "output = net(img)\n",
    "print(output.size())\n",
    "print(output[..., 12:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord:float, lambda_no_obj:float):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_no_obj = lambda_no_obj\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\").cuda()\n",
    "\n",
    "        self.scale_center = torch.tensor([128,72], dtype=torch.float32).to(device)\n",
    "        \n",
    "\n",
    "    # predictions: (72,128,)\n",
    "    def forward(self, predictions:torch.Tensor, target:torch.Tensor):\n",
    "        predictions = predictions.flatten(end_dim=-2)\n",
    "        \n",
    "        target_center_cell_id = torch.mul(target[..., 0:2], self.scale_center).floor().int().to(device)\n",
    "        flat_id = target_center_cell_id.mul(torch.tensor([1,128], dtype=torch.int32).to(device)).sum(1).to(device)\n",
    "\n",
    "        predicted_targets = predictions[flat_id].to(device)\n",
    "\n",
    "        # ==================== #\n",
    "        #       BOX LOSS       #\n",
    "        # ==================== #\n",
    "        box_predictions, box_targets = predicted_targets[..., 13:17], target[..., 0:4]\n",
    "        box_predictions[..., 2:4] = torch.sqrt(box_predictions[..., 2:4] + 1e-6) * 10 # avoids numerical issues since square root derivative is 1/x\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4]) * 10\n",
    "        box_predictions[..., 0] = box_predictions[..., 0] * 128\n",
    "        box_predictions[..., 1] = box_predictions[..., 1] * 72\n",
    "        box_targets[..., 0] = box_targets[..., 0] * 128\n",
    "        box_targets[..., 1] = box_targets[..., 1] * 72\n",
    "\n",
    "        box_loss = self.mse(box_predictions, box_targets)\n",
    "        \n",
    "        # ==================== #\n",
    "        #       OBJ LOSS       #\n",
    "        # ==================== #\n",
    "        obj_loss = self.mse(predicted_targets[..., 12], torch.ones(predicted_targets.size(0)).to(device))\n",
    "\n",
    "        # ==================== #\n",
    "        #     NO OBJ LOSS      #\n",
    "        # ==================== #\n",
    "        no_obj_ids = torch.ones(predictions.size()[0], dtype=torch.bool).to(device)\n",
    "        no_obj_ids[flat_id] = 0\n",
    "        predictions[flat_id, 12] = 0\n",
    "        no_obj_loss = self.mse(predictions[..., 12], torch.zeros(predictions.size(0)).to(device))\n",
    "\n",
    "        # ==================== #\n",
    "        #      CLASS LOSS      #\n",
    "        # ==================== #\n",
    "        class_target = torch.zeros([predicted_targets.size()[0], 12], dtype=torch.float32).to(device)\n",
    "        class_target[torch.arange(0, predicted_targets.size()[0]), target[...,4].int()] = 1\n",
    "        class_loss = self.mse(predicted_targets[..., :12], class_target)\n",
    "\n",
    "        loss = self.lambda_coord * box_loss + obj_loss + self.lambda_no_obj * no_obj_loss + class_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Loss function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CustomLoss(lambda_coord=1.0, lambda_no_obj=1).cuda()\n",
    "\n",
    "emu_target = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.1, 0.0, 0.0, 10],\n",
    "        [0.9999999, 0.9999999, 0.0, 0.0, 5]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "scale_center = torch.tensor([128,72], dtype=torch.float32)\n",
    "target_center_cell_id = emu_target[..., 0:2].mul(scale_center).floor().int()\n",
    "flat_id = target_center_cell_id.mul(torch.tensor([1,128], dtype=torch.int32)).sum(1)\n",
    "\n",
    "emu_output = torch.zeros([128*72,12+1+4])\n",
    "emu_output[flat_id[0], 13:17] = emu_target[0, 0:4]\n",
    "emu_output[flat_id[1], 13:17] = emu_target[1, 0:4]\n",
    "emu_output[flat_id, 12] = 1\n",
    "\n",
    "emu_output[flat_id[0], 10] = 1\n",
    "emu_output[flat_id[1], 5] = 1\n",
    "\n",
    "emu_output[[0,1,2], [3,4,5]] = 1 # correct(no effect on result since the respective region does not contain any objects)\n",
    "#emu_output[[0,1,2], 12] = 1\n",
    "\n",
    "emu_output, emu_target = emu_output.to(device), emu_target.to(device)\n",
    "\n",
    "#print(emu_output[flat_id])\n",
    "#print(emu_target)\n",
    "\n",
    "loss = loss_function(emu_output,emu_target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(train_loader, model, optimizer, loss_function):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x,y) in enumerate(loop):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            x_minibatch, y_minibatch = x[i], y[i]\n",
    "            x_minibatch, y_minibatch = x_minibatch, shorten_annotations_tensor(y_minibatch)\n",
    "\n",
    "            out = model(x_minibatch)\n",
    "            loss = loss_function(out, y_minibatch)\n",
    "            mean_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #update progress-bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = \"models/Net_165epochs_5-05\"\n",
    "if os.path.isfile(model_to_load):\n",
    "    model = torch.load(model_to_load)\n",
    "    model.eval()\n",
    "else:\n",
    "    model = SmallNet().to(device)\n",
    "    do_training = True\n",
    "\n",
    "if do_training:\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    loss_function = CustomLoss(lambda_coord=lambda_coord,lambda_no_obj=lambda_no_obj).cuda()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Training Epoch \" + str(epoch+1))\n",
    "        train_function(train_loader, model, optimizer, loss_function)\n",
    "\n",
    "        #if (epoch+1) % 15 == 0:\n",
    "        #    model_name = \"models/150epochs_\" + str(epoch+1)\n",
    "        #    torch.save(model, model_name)\n",
    "    \n",
    "    torch.save(model, \"models/latest_model\")\n",
    "    model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes_without_low_conf(net_output:torch.Tensor, confidence_threshold:float) -> list[list]:\n",
    "\n",
    "    # PARTIAL NON-MAX SUPRESSION DONE HERE\n",
    "    # first remove under-threshold bounding boxes from total(faster doing this with tensors than lists)\n",
    "    conf_t = torch.tensor(confidence_threshold, dtype=torch.float32)\n",
    "    net_output = net_output[net_output[..., 12]  > conf_t]\n",
    "\n",
    "    # select only one class for each one of the remaining/high-confidence boxes\n",
    "    boxes_data = torch.zeros([net_output.size()[0], 6], dtype=torch.float32)\n",
    "    boxes_data[..., 0] = net_output[..., 0:12].argmax(1)\n",
    "    boxes_data[..., 1] = net_output[..., 12]\n",
    "    bbox_wh_half = torch.mul(net_output[..., 15:], 0.5)\n",
    "    boxes_data[..., 2:4] = torch.sub(net_output[..., 13:15], bbox_wh_half)\n",
    "    boxes_data[..., 4:6] = torch.add(net_output[..., 13:15], bbox_wh_half)\n",
    "    \n",
    "    return boxes_data.tolist()\n",
    "\n",
    "def get_boxes_from_labels(labels:torch.Tensor) -> list[list]:\n",
    "\n",
    "    boxes_data = torch.zeros([labels.size()[0], 6], dtype=torch.float32)\n",
    "    boxes_data[..., 0] = labels[..., 4]\n",
    "    boxes_data[..., 1] = 1\n",
    "    bbox_wh_half = torch.mul(labels[..., 2:4], 0.5)\n",
    "    boxes_data[..., 2:4] = torch.sub(labels[..., 0:2], bbox_wh_half)\n",
    "    boxes_data[..., 4:6] = torch.add(labels[..., 0:2], bbox_wh_half)\n",
    "\n",
    "    return boxes_data.tolist()\n",
    "\n",
    "def get_boxes(model, loader, iou_threshold:float, confidence_threshold:float) -> tuple[list,list]:\n",
    "    train_idx = 0\n",
    "\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        \n",
    "        x = x.to(device)\n",
    "        #labels = shorten_annotations_tensor(labels)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            true_bboxes = get_boxes_from_labels(shorten_annotations_tensor(labels[idx]))\n",
    "            bboxes = get_boxes_without_low_conf(predictions[idx], confidence_threshold)\n",
    "            \n",
    "            nms_boxes = non_max_iou_suppression(bboxes, iou_threshold=iou_threshold)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes:\n",
    "                # many will get converted to 0 pred\n",
    "                #if box[1] > threshold:\n",
    "                all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    return all_pred_boxes, all_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get boxes into list of lists. Also apply nms on predicted values\n",
    "#pred_boxes, target_boxes = get_boxes(model=model, loader=test_loader, iou_threshold=iou_threshold, confidence_threshold=confidence_threshold)\n",
    "\n",
    "# perform mAP\n",
    "#mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=iou_threshold, num_classes=12)\n",
    "#print(f\"Test mAP: {mean_avg_prec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = test_set\n",
    "\n",
    "index = random.randint(0, len(dataset)-1)\n",
    "img, labels = dataset.__getitem__(index)\n",
    "img = img.to(device)\n",
    "\n",
    "prediction = model(img).to(\"cpu\")\n",
    "\n",
    "bboxes = get_boxes_without_low_conf(prediction, confidence_threshold)\n",
    "            \n",
    "bboxes = torch.tensor(non_max_iou_suppression(bboxes, iou_threshold=iou_threshold))\n",
    "\n",
    "img, labels = dataset.__getitem__(index, mode_plot=True)\n",
    "img_draw = ImageDraw.Draw(img)\n",
    "bboxes = bboxes[...,2:] * torch.tensor([1280,720,1280,720])\n",
    "\n",
    "for i in range(bboxes.shape[0]):\n",
    "    bbox = bboxes[i]\n",
    "\n",
    "    x0 = bbox[0]\n",
    "    x1 = bbox[2]\n",
    "    y0 = bbox[1]\n",
    "    y1 = bbox[3]\n",
    "\n",
    "    img_draw.rectangle([x0, y0, x1, y1], outline=\"red\")\n",
    "        \n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resize = transforms.Compose([transforms.Resize([720,1280])])\n",
    "real_set = RisikoDataset(dataset_dir=dataset_path, mode=\"real\", transform=img_resize)\n",
    "real_loader = torch.utils.data.DataLoader(real_set, batch_size=1, shuffle=False, num_workers=1, pin_memory=pin_memory, drop_last=True)\n",
    "\n",
    "def evaluate(model, dataset, dataloader):\n",
    "    # get boxes into list of lists. Also apply nms on predicted values\n",
    "    pred_boxes, target_boxes = get_boxes(model=model, loader=dataloader, iou_threshold=iou_threshold, confidence_threshold=confidence_threshold)\n",
    "\n",
    "    # perform mAP\n",
    "    mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=iou_threshold, num_classes=12)\n",
    "    print(f\"mAP: {mean_avg_prec}\")\n",
    "\n",
    "    index = random.randint(0, len(dataset)-1)\n",
    "    img, labels = dataset.__getitem__(index)\n",
    "    print(img.size())\n",
    "    img = img.to(device)\n",
    "\n",
    "    prediction = model(img).to(\"cpu\")\n",
    "\n",
    "    bboxes = get_boxes_without_low_conf(prediction, confidence_threshold=confidence_threshold)\n",
    "                \n",
    "    bboxes = torch.tensor(non_max_iou_suppression(bboxes, iou_threshold=iou_threshold))\n",
    "\n",
    "    img, labels = dataset.__getitem__(index, mode_plot=True)\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    bboxes = bboxes[...,2:] * torch.tensor([1280,720,1280,720])\n",
    "\n",
    "    for i in range(bboxes.shape[0]):\n",
    "        bbox = bboxes[i]\n",
    "\n",
    "        x0 = bbox[0]\n",
    "        x1 = bbox[2]\n",
    "        y0 = bbox[1]\n",
    "        y1 = bbox[3]\n",
    "\n",
    "        img_draw.rectangle([x0, y0, x1, y1], outline=\"red\")\n",
    "            \n",
    "    display(img)\n",
    "\n",
    "evaluate(model, test_set, test_loader)\n",
    "evaluate(model, real_set, real_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
